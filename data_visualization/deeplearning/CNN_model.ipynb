{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "414/414 [==============================] - 10s 24ms/step - loss: 0.8798 - accuracy: 0.5692 - val_loss: 0.5248 - val_accuracy: 0.8181\n",
      "Epoch 2/5\n",
      "414/414 [==============================] - 12s 28ms/step - loss: 0.5054 - accuracy: 0.8063 - val_loss: 0.2755 - val_accuracy: 0.9184\n",
      "Epoch 3/5\n",
      "414/414 [==============================] - 12s 28ms/step - loss: 0.3065 - accuracy: 0.8991 - val_loss: 0.1597 - val_accuracy: 0.9526\n",
      "Epoch 4/5\n",
      "414/414 [==============================] - 12s 30ms/step - loss: 0.1899 - accuracy: 0.9419 - val_loss: 0.0976 - val_accuracy: 0.9717\n",
      "Epoch 5/5\n",
      "414/414 [==============================] - 13s 31ms/step - loss: 0.1325 - accuracy: 0.9622 - val_loss: 0.0650 - val_accuracy: 0.9801\n",
      "60/60 [==============================] - 0s 2ms/step - loss: 0.0646 - accuracy: 0.9772\n",
      "accuracy : 97.715735\n",
      "loss : 0.064623 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "\n",
    "train_file = './ChatbotData .csv'\n",
    "data =pd.read_csv(train_file, delimiter =',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "word_index= tokenizer.word_index\n",
    "\n",
    "\n",
    "MAX_SEQ_LEN = 15\n",
    "#pad_sequences -> 남은 공간 0으로 채워주기\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding = 'post')\n",
    "\n",
    "#모델 생성\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
    "ds = ds.shuffle(len(features))\n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "#배치\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "#하이퍼파라미터\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(word_index) + 1\n",
    "\n",
    "#CNN 모델정의\n",
    "\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length= MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1= Conv1D(filters = 128, kernel_size = 3, padding = 'valid', activation = tf.nn.relu)(dropout_emb)\n",
    "pool1= GlobalMaxPool1D()(conv1)\n",
    "conv2 = Conv1D(filters = 128, kernel_size = 4, padding = 'valid',activation= tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(filters = 128, kernel_size = 5, padding = 'valid',activation= tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "#pool1,2,3합치기\n",
    "concat = concatenate([pool1,pool2,pool3]) # 여러개는 리스트형태으로\n",
    "\n",
    "hidden = Dense(128,activation = tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate = dropout_prob)(hidden)\n",
    "logits = Dense(3, name = 'logits')(dropout_hidden)\n",
    "predictions = Dense(3, activation = tf.nn.softmax)(logits)\n",
    "\n",
    "model = Model(inputs =input_layer, outputs= predictions)\n",
    "model.compile(optimizer = 'adam', loss='sparse_categorical_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_ds, validation_data = val_ds, epochs = EPOCH, verbose = 1)\n",
    "loss, accuracy = model.evaluate(test_ds, verbose = 1)\n",
    "print('accuracy : %f' %(accuracy * 100))\n",
    "print('loss : %f ' %(loss))\n",
    "\n",
    "model.save('cnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 15, 128)      1715072     input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 15, 128)      0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 13, 128)      49280       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 12, 128)      65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 11, 128)      82048       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 128)          0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 128)          0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_22 (Global (None, 128)          0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 384)          0           global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "                                                                 global_max_pooling1d_22[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          49280       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 128)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "logits (Dense)                  (None, 3)            387         dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 3)            12          logits[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,961,743\n",
      "Trainable params: 1,961,743\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "100/100 - 0s - loss: 0.0724 - accuracy: 0.4565\n",
      "단어 시퀸스 :  ['사랑한다고', '말해주면', '뭐가', '덧나나']\n",
      "단어 인덱스 시퀸스 :  [  601 12016   152 12017     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n",
      "문장 분류(정답):  2\n",
      "감정 예측 점수 :  [[0.01583751 0.00191524 0.9822473 ]]\n",
      "감정 예측 클래스 :  [2]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "train_file = './ChatbotData .csv'\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "\n",
    "MAX_SEQ_LEN = 15\n",
    "#pad_sequences -> 남은 공간 0으로 채워주기\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding = 'post')\n",
    "\n",
    "#모델 생성\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
    "ds = ds.shuffle(len(features))\n",
    "test_ds = ds.take(2000).batch(20) #test 데이터셋 2000개\n",
    "\n",
    "model = load_model('cnn_model.h5')\n",
    "model.summary()\n",
    "model.evaluate(test_ds,verbose = 2)\n",
    "\n",
    "print(\"단어 시퀸스 : \", corpus[10000])\n",
    "print(\"단어 인덱스 시퀸스 : \", padded_seqs[10000])\n",
    "print(\"문장 분류(정답): \", labels[10000])\n",
    "\n",
    "picks= [10000]\n",
    "predict = model.predict(padded_seqs[picks])\n",
    "predict_class = tf.math.argmax(predict, axis = 1)\n",
    "print(\"감정 예측 점수 : \", predict)\n",
    "print(\"감정 예측 클래스 : \", predict_class.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
