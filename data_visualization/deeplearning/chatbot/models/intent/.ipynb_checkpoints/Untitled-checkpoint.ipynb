{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "Epoch 1/5\n",
      "3698/3698 [==============================] - 106s 29ms/step - loss: 0.0525 - accuracy: 0.9829 - val_loss: 0.0144 - val_accuracy: 0.9928\n",
      "Epoch 2/5\n",
      "3698/3698 [==============================] - 107s 29ms/step - loss: 0.0156 - accuracy: 0.9948 - val_loss: 0.0094 - val_accuracy: 0.9932\n",
      "Epoch 3/5\n",
      "3698/3698 [==============================] - 104s 28ms/step - loss: 0.0122 - accuracy: 0.9957 - val_loss: 0.0066 - val_accuracy: 0.9974\n",
      "Epoch 4/5\n",
      "3698/3698 [==============================] - 113s 30ms/step - loss: 0.0093 - accuracy: 0.9964 - val_loss: 0.0070 - val_accuracy: 0.9971\n",
      "Epoch 5/5\n",
      "3480/3698 [===========================>..] - ETA: 6s - loss: 0.0087 - accuracy: 0.9963"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "import jpype\n",
    "from konlpy.tag import Komoran\n",
    "import pickle\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, word2index_dic='', userdic=None):\n",
    "        print('hi')\n",
    "        # 단어 인덱스 사전 불러오기\n",
    "        if(word2index_dic != ''):\n",
    "            f = open(word2index_dic, \"rb\")\n",
    "            self.word_index = pickle.load(f)\n",
    "            f.close()\n",
    "        else:\n",
    "            self.word_index = None\n",
    "\n",
    "        # 형태소 분석기 초기화\n",
    "        self.komoran = Komoran(userdic=userdic)\n",
    "\n",
    "        # 제외할 품사\n",
    "        # 참조 : https://docs.komoran.kr/firststep/postypes.html\n",
    "        # 관계언 제거, 기호 제거\n",
    "        # 어미 제거\n",
    "        # 접미사 제거\n",
    "        self.exclusion_tags = [\n",
    "            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',\n",
    "            'JX', 'JC',\n",
    "            'SF', 'SP', 'SS', 'SE', 'SO',\n",
    "            'EP', 'EF', 'EC', 'ETN', 'ETM',\n",
    "            'XSN', 'XSV', 'XSA'\n",
    "        ]\n",
    "\n",
    "    # 형태소 분석기 POS 태거\n",
    "    def pos(self, sentence):\n",
    "        jpype.attachThreadToJVM()\n",
    "        return self.komoran.pos(sentence)\n",
    "\n",
    "    # 불용어 제거 후, 필요한 품사 정보만 가져오기\n",
    "    def get_keywords(self, pos, without_tag=False):\n",
    "        f = lambda x: x in self.exclusion_tags\n",
    "        word_list = []\n",
    "        for p in pos:\n",
    "            if f(p[1]) is False:\n",
    "                word_list.append(p if without_tag is False else p[0])\n",
    "        return word_list\n",
    "\n",
    "    # 키워드를 단어 인덱스 시퀀스로 변환\n",
    "    def get_wordidx_sequence(self, keywords):\n",
    "        if self.word_index is None:\n",
    "            return []\n",
    "\n",
    "        w2i = []\n",
    "        for word in keywords:\n",
    "            try:\n",
    "                w2i.append(self.word_index[word])\n",
    "            except KeyError:\n",
    "                # 해당 단어가 사전에 없는 경우, OOV 처리\n",
    "                w2i.append(self.word_index['OOV'])\n",
    "        return w2i\n",
    "    \n",
    "train_file = \"total_train_data.csv\"\n",
    "data= pd.read_csv(train_file, delimiter=',')\n",
    "queries = data['query'].tolist()\n",
    "intents = data['intent'].tolist()\n",
    "\n",
    "p = Preprocess(word2index_dic = '../../train_tools/dict/chatbot_dict.bin',\n",
    "               userdic='../../utils/user_dict.tsv')\n",
    "\n",
    "sequences = []\n",
    "for sentence in queries:\n",
    "    pos= p.pos(sentence)\n",
    "    keywords = p.get_keywords(pos, without_tag = True)\n",
    "    seq = p.get_wordidx_sequence(keywords)\n",
    "    sequences.append(seq)\n",
    "\n",
    "from GlobalParams import MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen = MAX_SEQ_LEN, padding = 'post')\n",
    "\n",
    "#학습용데이터셋 생성\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, intents))\n",
    "ds = ds.shuffle(len(queries))\n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.word_index) + 1\n",
    "\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length= MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate= dropout_prob)(embedding_layer)\n",
    "#학습모델\n",
    "conv1 = Conv1D(filters = 128, kernel_size =3, padding = 'valid',activation = tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(filters = 128, kernel_size =4, padding = 'valid',activation = tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(filters = 128, kernel_size =5, padding = 'valid',activation = tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "concat = concatenate([pool1,pool2,pool3]) #합치기\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(5, name='logits')(dropout_hidden)\n",
    "predictions = Dense(5, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "model = Model(inputs= input_layer, outputs = predictions)\n",
    "model.compile(optimizer = 'adam', loss ='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_ds, validation_data = val_ds, epochs = EPOCH, verbose = 1)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_ds, verbose = 1)\n",
    "print('Accuracy : %f' %(accuracy * 100))\n",
    "print('loss : %f' %(loss))\n",
    "\n",
    "model.save('intent_model.h5')\n",
    "print('finish')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocess import Preprocess\n",
    "\n",
    "p = Preprocess(               userdic='../../utils/user_dic.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GlobalParams import MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "import pickle\n",
    "import jpype\n",
    "\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, word2index_dic='', userdic=None):\n",
    "        print('hi')\n",
    "        # 단어 인덱스 사전 불러오기\n",
    "        if(word2index_dic != ''):\n",
    "            f = open(word2index_dic, \"rb\")\n",
    "            self.word_index = pickle.load(f)\n",
    "            f.close()\n",
    "        else:\n",
    "            self.word_index = None\n",
    "\n",
    "        # 형태소 분석기 초기화\n",
    "        self.komoran = Komoran(userdic=userdic)\n",
    "\n",
    "        # 제외할 품사\n",
    "        # 참조 : https://docs.komoran.kr/firststep/postypes.html\n",
    "        # 관계언 제거, 기호 제거\n",
    "        # 어미 제거\n",
    "        # 접미사 제거\n",
    "        self.exclusion_tags = [\n",
    "            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',\n",
    "            'JX', 'JC',\n",
    "            'SF', 'SP', 'SS', 'SE', 'SO',\n",
    "            'EP', 'EF', 'EC', 'ETN', 'ETM',\n",
    "            'XSN', 'XSV', 'XSA'\n",
    "        ]\n",
    "\n",
    "    # 형태소 분석기 POS 태거\n",
    "    def pos(self, sentence):\n",
    "        jpype.attachThreadToJVM()\n",
    "        return self.komoran.pos(sentence)\n",
    "\n",
    "    # 불용어 제거 후, 필요한 품사 정보만 가져오기\n",
    "    def get_keywords(self, pos, without_tag=False):\n",
    "        f = lambda x: x in self.exclusion_tags\n",
    "        word_list = []\n",
    "        for p in pos:\n",
    "            if f(p[1]) is False:\n",
    "                word_list.append(p if without_tag is False else p[0])\n",
    "        return word_list\n",
    "\n",
    "    # 키워드를 단어 인덱스 시퀀스로 변환\n",
    "    def get_wordidx_sequence(self, keywords):\n",
    "        if self.word_index is None:\n",
    "            return []\n",
    "\n",
    "        w2i = []\n",
    "        for word in keywords:\n",
    "            try:\n",
    "                w2i.append(self.word_index[word])\n",
    "            except KeyError:\n",
    "                # 해당 단어가 사전에 없는 경우, OOV 처리\n",
    "                w2i.append(self.word_index['OOV'])\n",
    "        return w2i\n",
    "p = Preprocess(word2index_dic = '../../train_tools/dict/chatbot_dict.bin',\n",
    "               userdic='../../utils/user_dict.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "class IntentModel:\n",
    "    def __init__(self, model_name, proprocess):\n",
    "        self.labels= {0:\"인사\",1:\"욕설\",2:\"주문\",3:\"예약\",4:\"기타\"} #의도분류딕셔너리\n",
    "        self.model = load_model(model_name)\n",
    "\n",
    "        self.p = proprocess\n",
    "\n",
    "    def predicT_class(self, query):\n",
    "        pos = self.p.pos(query)\n",
    "        keywords=  self.p.get_keywords(pos, without_tag = True)\n",
    "        sequence = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        from GlobalParams import MAX_SEQ_LEN\n",
    "\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen = MAX_SEQ_LEN, padding = 'post')\n",
    "        predict = self.model.predict(padded_seqs)\n",
    "        predict_class = tf.math.argmax(predict, axis = 1)\n",
    "        return predict_class.numpy()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
